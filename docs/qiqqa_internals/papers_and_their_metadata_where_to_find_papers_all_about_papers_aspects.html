<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <h1>Where to find papers</h1>
<ul>
<li><a href="https://core.ac.uk/">https://core.ac.uk/</a></li>
<li>…</li>
</ul>
<h1>Metadata / Attributes</h1>
<ul>
<li>
<p>a paper can be found on multiple sites, hence MANY URLs as source.</p>
<ul>
<li>some of these may be ‘dead’ by the time the reader of the reference (in your own paper/document, which references it), so it is paramount to have:
<ul>
<li>ability to store and ADD source URLs at any time</li>
<li>check the validity / availability of a URL at any time
<ul>
<li>Note here that some URLs behave <em>oddly</em> and a simple <code>curl</code>-style download test might not give the correct answer: see <a href="./testing_nasty_urls_for_pdfs.html">Testing: nasty URLs for PDFs</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>metadata may be sourced from various locations, including</p>
<ul>
<li>BibTeX sources (Scholar for example)</li>
<li>RIS/… sources</li>
<li>XML sources (MDPI for example)</li>
<li>Inferral from HTML page content (DOI, ArXiv, etc.)</li>
<li>ADS (Windows: some browsers store the source URL in an ‘Alternative Stream’ on NTFS: very useful for us to extract that metadata and include it in the document record as it automatically lists the URL where the file has been downloaded, irrespective of any file move/copy/rename actions in between the time it was downloaded and the moment it was imported into Qiqqa.)</li>
</ul>
<p>The question here is: do we wish to store/track the <em>imported</em> metadata next to the final mixed output?</p>
<p>My answer would be YES, as I like to see what came from where, like in a VCS (git, …), when you investigate the correctness of metadata: a task which occurs too often to ignore IMO.</p>
</li>
<li>
<p>Annotations are a feature in high demand. Qiqqa has annotations, but then there’s a plethora of annotation import/export/enhancements that it currently DOES NOT have (Oct 2020)</p>
</li>
<li>
<p>papers can have MANY</p>
<ul>
<li>
<p>duplicates (exact copies)</p>
<p>Qiqqa prevents this by content-hash matching them, so that would result in multiple import source URIs for a single paper</p>
</li>
<li>
<p>near duplicates Type S (‘Sources’)</p>
<p>These have <em>very probably</em> the exact same content, but with different <em>leader pages</em>. I’ve found this with</p>
<ul>
<li>
<p>researchgate website</p>
</li>
<li>
<p>some Korean universities which prepend their own leader page on everything</p>
</li>
<li>
<p>electronic datasheets</p>
<p>Where it gets worse, by the way, is when a company is bought by another and you happen to find the document published in the few years afterwards, i.e. in their transitional period. Then you can easily end up with TWO OR MORE leader pages before you hit the actual content. On-Semi/Fairchild is one such example.</p>
<p>Another occasion where you may observe this is when you get a datasheet off a generic datasheet finder/provider website or <em>component seller</em>, which appends their own, often oddly sized, <em>additional</em> leader pages.</p>
</li>
</ul>
</li>
<li>
<p>near duplicates Type T (‘Typesetting’)</p>
<p>Really another Type S, but you’ll find this quite often with some older papers (typewriter format and magazine preprint of the same paper: Courier vs. proportional serif font layout) and older datasheets (Toshiba, RCA)</p>
<p>This occurs most often with datasheets when you happen to find the same datasheet for multiple publication years (electron tube manuals, etc.)</p>
</li>
<li>
<p>near duplicates Type C (‘Content’)</p>
<p>This is more relevant for scientific papers, where you may happen to find all or some of this set:</p>
<ul>
<li>preprint</li>
<li>personal author copy (which may be different for different authors when it’s a co-authored paper)</li>
<li>MULTIPLE magazine prints (though they want exclusivity, some papers have been published in multiple publications, e.g. magazine, symposium/summit syllabi, reprint in a later year for historical papers)</li>
<li>abridged versions for popular press or special magazine issues and/or press releases</li>
<li>corrected versions which have been updated and reprinted after processing of critique</li>
<li>versions marked <strong>RETRACTED</strong>: this is really a kind of Type T and are rare as most papers are VERY SILENTLY retracted: you need to spend quite some effort to make sure (Retraction Watch site, …). There’s some notorious stuff in the social-psychology corner that got retracted after it finally came out the world-renowned Dutch researcher had invented his “meat eaters are more anti-social than vegatarians” data wholesale, for example. See (this New York Times article)[<a href="https://www.nytimes.com/2013/04/28/magazine/diederik-stapels-audacious-academic-fraud.html">https://www.nytimes.com/2013/04/28/magazine/diederik-stapels-audacious-academic-fraud.html</a>]. There’s a lot more of that: check Retraction Watch site.</li>
<li>some papers happen to be <strong>plagiarized</strong> in other journals by other authors. Sometimes this is hard to detect, sometimes it’s blatantly obvious. I’ve seen some Indian journals publish stuff with Indian authors listed, which can also be found with different journals and different authors (Chineze, American, etc.), but this is not ‘an Indian thing’: it’s just that I happen to encounter it the first few times with PDFs from more or less ‘obscure’ Indian journals.</li>
<li>a more difficult to categorize variant of a paper is one where the same dataset and part of the text is re-used by the same authors to look at maybe a different aspect in that research or something else. I’ve encountered a few like that: these are not really <em>duplicates</em> but there the papers have a <em>major overlap</em>.</li>
</ul>
</li>
</ul>
<p>I guess it is, like so much else in life, a continuous gradient from black to white when it comes to <em>duplicates</em>: the question to ask when reading a new paper instance is “how much of a <em>duplicate</em> are you then?”</p>
</li>
<li>
<p>people like to refer to papers with a shorthand reference, which should be <em>unique on a per-publication bases</em> at least.</p>
<p>What I mean here is that (BibTeX) metadata always comes with a reference shorthand, e.g. <code>[Knuth78]</code>, but folks have preferences, so this may not always suite your taste or the incoming metadata reference code may look contrived or senseless/random, e.g. <code>[Bur7X1q2]</code> and people will want to be able to apply their own ‘naming scheme’ for this.</p>
<p>The key here is that this is usually done on a <em>per-library</em> basis, where the generated shorthand should thusly be unique across the entire <em>library</em>, where the references <em>really</em> only need to be unique in your own production, i.e. in the paper/document/book you are writing yourself, in which you drop these articles as references!</p>
<p>Here one could go and create a Qiqqa library for every project, clone/link documents into that project library from your main collection library (currently Qiqqa only supports COPY or MOVE, but not LINK/REFERENCE, by the way) and then create/generate a unique reference label set for that reduced collection and use it in your paper. Or you could create a Project and have Project-specific metadata alterations/augmentations, like the reference shorthand code, kept as a separate ‘overlay’? (Qiqqa currently does not have any such ‘library metadata overlay/layering’ facility: this is just us thinking about what should be / could be for optimal usability.)</p>
</li>
<li>
<p>people also have requested the ‘sane filename and/or keep my personal file organization’ as a feature over the years. (#205, …)</p>
<p>While Qiqqa can create a <strong>library export</strong> where every PDF paper gets a sane name (author, title) asnd is referenced by a HTML index sheet, this is not really the same thing.</p>
<p>The question here is: can we give people access to the Qiqqa library files via ‘sane names’ (which, again, are a matter of <em>taste</em>!) while the Qiqqa machinery can keep track of those PDFs and everything else using the content hash based scheme it already uses? (Content hash based VCS like <code>git</code> work alike, but the goal/purpose is different, hence you always have a ‘working copy’ of a file in <code>git</code> versus the archived versions, which are stored on disk in a git-specific database structure. Qiqqa does not need a ‘working copy’ (more on multiple copies of the same document later, though) as it’s basically an <em>archival</em> system: it’s your local librarian, so any ‘working copy’ is not managed by Qiqqa and is <em>not meant to be managed by Qiqqa</em> as that ‘working copy’ would not be a ready (published or unpublished) paper yet!)</p>
<p>I believe the technology answer there is using hardlinks, which are available in NTFS / Windows and on all UNIX filesystems: Qiqqa can maybe keep a master file in the repository, using the standard content hash based naming scheme, while it may keep hardlinks or softlinks to those ‘sensibly named’ imported source files and/or ‘sensibly named’ copies which the user wishes to see and use. As lonmg as it all stays on a single drive in Windows (NTFS), you’re golden with hardlinks, but it gets tricky and bothersome when you move those copies onto a different drive… There’s some stuff to consider there.</p>
</li>
<li>
<p>I have found many PDFs which needed processing to make them display or otherwise work correctly (OCR!) in Qiqqa.</p>
<p>While I use tools like <code>qpdf</code> and <code>mupdf</code> for some of these tasks, the important subject here is: <em>how do we keep track of the various file versions/copies</em> here?</p>
<p>I suppose this is another type of <em>duplicate</em> which we will dub <em>Type V</em> (‘View[ability]’) for now: a single imported PDF would become, after all the processing, a series of semi-duplicates:</p>
<ul>
<li>the original, imported, PDF file</li>
<li>the unlocked PDF file (<code>qpdf</code>) which will render and print properly at least</li>
<li>the OCR’ed PDF file which has gained a text layer if the original didn’t have one (or has an incomplete or considered-wrong-by-the-user one)</li>
<li>the PDF file which has everything above plus embedded annotations perhaps? Or attachments? – Do we really want this variant in the library or do we wish to generate it on the fly every time a user requests it?</li>
</ul>
<p>As these variants all will have quite different content hashes, there’s the additional task for Qiqqa to link up multiple content hashes as ‘technical duplicates’ of the paper. Hence my take on this as calling this another type of (semi-)duplicate: all those semi-duplicates discussed above suffer from the same ailment: different content hashes for the same base content due to slew of different reasons.</p>
<p>Qiqqa currently (Oct 2020) only supports “one document, one content hash”, but this must be turned into something where multiple PDF files can be linked up and ‘clustered’ into a single overarching item, while the different <em>files</em> <em>will</em> have slightly different metadata in some places, e.g. their ‘state’ or ‘type’ or whatever you wish to call it:</p>
<ul>
<li>
<p>is this the imported original?</p>
</li>
<li>
<p>is this a copy of X, which has had superfluous leader pages stripped off?</p>
</li>
<li>
<p>is this a copy of X, which has been unlocked for regular viewing and printing?</p>
</li>
<li>
<p>is this a copy of X, which has a searchable text layer?</p>
<ul>
<li>
<p>has this text layer been vetted by the user as ‘good’/‘use this!’?</p>
<p>e.g. some (older) TeX output PDFs have a non-Unicode character encoding which does not convert to <strong>intelligible</strong> searchable text: it is quite useful to have these carry an additional or maybe altered text layer which can be extracted to produce legible text easily.</p>
<p>Some TeX files also seem to have lost all their whitespace in the text extract, so some more work needs to be done on the tools anyway and an additional text layer might help.</p>
</li>
<li>
<p>is this a text layer in a language that you can read / <em>comprehend</em>?</p>
<p>Then there are the ‘foreign’ publications, e.g. Chinese papers in my particualr case, which require <strong>language translation</strong> to become intelligible to me.
Once again, an additional text layer might come in handy to show me a translated overlay of the content. When this translation is stored in the PDF, we end up with a <em>technically different content</em> and thus a different file with a different content hash.</p>
</li>
</ul>
<p>And leading up to that, there’s:</p>
<ul>
<li>has this text layer been vetted/rated by the automation in Qiqqa? (Automatic rating / TOC building / …whatever…)</li>
<li>has this text layer been (auto-)translated by the automation in Qiqqa to your language(s) of choice?</li>
</ul>
<p>As the text layer would quite probably be obtained through the OCR process, we may also want to store/track the OCR and OCR preprocessing setup which resulted in this text layer: what have we done with the original to produce this?</p>
<p>Though one might want to store that sort of info in a separate process configuration file or database record, instead of including it <em>inside</em> the PDF; the resulting text layer <em>will</em> be part of the new PDF anyhow!</p>
</li>
</ul>
</li>
</ul>

  </head>
  <body>
    
    <h1>Where to find papers</h1>
<ul>
<li><a href="https://core.ac.uk/">https://core.ac.uk/</a></li>
<li>…</li>
</ul>
<h1>Metadata / Attributes</h1>
<ul>
<li>
<p>a paper can be found on multiple sites, hence MANY URLs as source.</p>
<ul>
<li>some of these may be ‘dead’ by the time the reader of the reference (in your own paper/document, which references it), so it is paramount to have:
<ul>
<li>ability to store and ADD source URLs at any time</li>
<li>check the validity / availability of a URL at any time
<ul>
<li>Note here that some URLs behave <em>oddly</em> and a simple <code>curl</code>-style download test might not give the correct answer: see <a href="./testing_nasty_urls_for_pdfs.html">Testing: nasty URLs for PDFs</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>metadata may be sourced from various locations, including</p>
<ul>
<li>BibTeX sources (Scholar for example)</li>
<li>RIS/… sources</li>
<li>XML sources (MDPI for example)</li>
<li>Inferral from HTML page content (DOI, ArXiv, etc.)</li>
<li>ADS (Windows: some browsers store the source URL in an ‘Alternative Stream’ on NTFS: very useful for us to extract that metadata and include it in the document record as it automatically lists the URL where the file has been downloaded, irrespective of any file move/copy/rename actions in between the time it was downloaded and the moment it was imported into Qiqqa.)</li>
</ul>
<p>The question here is: do we wish to store/track the <em>imported</em> metadata next to the final mixed output?</p>
<p>My answer would be YES, as I like to see what came from where, like in a VCS (git, …), when you investigate the correctness of metadata: a task which occurs too often to ignore IMO.</p>
</li>
<li>
<p>Annotations are a feature in high demand. Qiqqa has annotations, but then there’s a plethora of annotation import/export/enhancements that it currently DOES NOT have (Oct 2020)</p>
</li>
<li>
<p>papers can have MANY</p>
<ul>
<li>
<p>duplicates (exact copies)</p>
<p>Qiqqa prevents this by content-hash matching them, so that would result in multiple import source URIs for a single paper</p>
</li>
<li>
<p>near duplicates Type S (‘Sources’)</p>
<p>These have <em>very probably</em> the exact same content, but with different <em>leader pages</em>. I’ve found this with</p>
<ul>
<li>
<p>researchgate website</p>
</li>
<li>
<p>some Korean universities which prepend their own leader page on everything</p>
</li>
<li>
<p>electronic datasheets</p>
<p>Where it gets worse, by the way, is when a company is bought by another and you happen to find the document published in the few years afterwards, i.e. in their transitional period. Then you can easily end up with TWO OR MORE leader pages before you hit the actual content. On-Semi/Fairchild is one such example.</p>
<p>Another occasion where you may observe this is when you get a datasheet off a generic datasheet finder/provider website or <em>component seller</em>, which appends their own, often oddly sized, <em>additional</em> leader pages.</p>
</li>
</ul>
</li>
<li>
<p>near duplicates Type T (‘Typesetting’)</p>
<p>Really another Type S, but you’ll find this quite often with some older papers (typewriter format and magazine preprint of the same paper: Courier vs. proportional serif font layout) and older datasheets (Toshiba, RCA)</p>
<p>This occurs most often with datasheets when you happen to find the same datasheet for multiple publication years (electron tube manuals, etc.)</p>
</li>
<li>
<p>near duplicates Type C (‘Content’)</p>
<p>This is more relevant for scientific papers, where you may happen to find all or some of this set:</p>
<ul>
<li>preprint</li>
<li>personal author copy (which may be different for different authors when it’s a co-authored paper)</li>
<li>MULTIPLE magazine prints (though they want exclusivity, some papers have been published in multiple publications, e.g. magazine, symposium/summit syllabi, reprint in a later year for historical papers)</li>
<li>abridged versions for popular press or special magazine issues and/or press releases</li>
<li>corrected versions which have been updated and reprinted after processing of critique</li>
<li>versions marked <strong>RETRACTED</strong>: this is really a kind of Type T and are rare as most papers are VERY SILENTLY retracted: you need to spend quite some effort to make sure (Retraction Watch site, …). There’s some notorious stuff in the social-psychology corner that got retracted after it finally came out the world-renowned Dutch researcher had invented his “meat eaters are more anti-social than vegatarians” data wholesale, for example. See (this New York Times article)[<a href="https://www.nytimes.com/2013/04/28/magazine/diederik-stapels-audacious-academic-fraud.html">https://www.nytimes.com/2013/04/28/magazine/diederik-stapels-audacious-academic-fraud.html</a>]. There’s a lot more of that: check Retraction Watch site.</li>
<li>some papers happen to be <strong>plagiarized</strong> in other journals by other authors. Sometimes this is hard to detect, sometimes it’s blatantly obvious. I’ve seen some Indian journals publish stuff with Indian authors listed, which can also be found with different journals and different authors (Chineze, American, etc.), but this is not ‘an Indian thing’: it’s just that I happen to encounter it the first few times with PDFs from more or less ‘obscure’ Indian journals.</li>
<li>a more difficult to categorize variant of a paper is one where the same dataset and part of the text is re-used by the same authors to look at maybe a different aspect in that research or something else. I’ve encountered a few like that: these are not really <em>duplicates</em> but there the papers have a <em>major overlap</em>.</li>
</ul>
</li>
</ul>
<p>I guess it is, like so much else in life, a continuous gradient from black to white when it comes to <em>duplicates</em>: the question to ask when reading a new paper instance is “how much of a <em>duplicate</em> are you then?”</p>
</li>
<li>
<p>people like to refer to papers with a shorthand reference, which should be <em>unique on a per-publication bases</em> at least.</p>
<p>What I mean here is that (BibTeX) metadata always comes with a reference shorthand, e.g. <code>[Knuth78]</code>, but folks have preferences, so this may not always suite your taste or the incoming metadata reference code may look contrived or senseless/random, e.g. <code>[Bur7X1q2]</code> and people will want to be able to apply their own ‘naming scheme’ for this.</p>
<p>The key here is that this is usually done on a <em>per-library</em> basis, where the generated shorthand should thusly be unique across the entire <em>library</em>, where the references <em>really</em> only need to be unique in your own production, i.e. in the paper/document/book you are writing yourself, in which you drop these articles as references!</p>
<p>Here one could go and create a Qiqqa library for every project, clone/link documents into that project library from your main collection library (currently Qiqqa only supports COPY or MOVE, but not LINK/REFERENCE, by the way) and then create/generate a unique reference label set for that reduced collection and use it in your paper. Or you could create a Project and have Project-specific metadata alterations/augmentations, like the reference shorthand code, kept as a separate ‘overlay’? (Qiqqa currently does not have any such ‘library metadata overlay/layering’ facility: this is just us thinking about what should be / could be for optimal usability.)</p>
</li>
<li>
<p>people also have requested the ‘sane filename and/or keep my personal file organization’ as a feature over the years. (#205, …)</p>
<p>While Qiqqa can create a <strong>library export</strong> where every PDF paper gets a sane name (author, title) asnd is referenced by a HTML index sheet, this is not really the same thing.</p>
<p>The question here is: can we give people access to the Qiqqa library files via ‘sane names’ (which, again, are a matter of <em>taste</em>!) while the Qiqqa machinery can keep track of those PDFs and everything else using the content hash based scheme it already uses? (Content hash based VCS like <code>git</code> work alike, but the goal/purpose is different, hence you always have a ‘working copy’ of a file in <code>git</code> versus the archived versions, which are stored on disk in a git-specific database structure. Qiqqa does not need a ‘working copy’ (more on multiple copies of the same document later, though) as it’s basically an <em>archival</em> system: it’s your local librarian, so any ‘working copy’ is not managed by Qiqqa and is <em>not meant to be managed by Qiqqa</em> as that ‘working copy’ would not be a ready (published or unpublished) paper yet!)</p>
<p>I believe the technology answer there is using hardlinks, which are available in NTFS / Windows and on all UNIX filesystems: Qiqqa can maybe keep a master file in the repository, using the standard content hash based naming scheme, while it may keep hardlinks or softlinks to those ‘sensibly named’ imported source files and/or ‘sensibly named’ copies which the user wishes to see and use. As lonmg as it all stays on a single drive in Windows (NTFS), you’re golden with hardlinks, but it gets tricky and bothersome when you move those copies onto a different drive… There’s some stuff to consider there.</p>
</li>
<li>
<p>I have found many PDFs which needed processing to make them display or otherwise work correctly (OCR!) in Qiqqa.</p>
<p>While I use tools like <code>qpdf</code> and <code>mupdf</code> for some of these tasks, the important subject here is: <em>how do we keep track of the various file versions/copies</em> here?</p>
<p>I suppose this is another type of <em>duplicate</em> which we will dub <em>Type V</em> (‘View[ability]’) for now: a single imported PDF would become, after all the processing, a series of semi-duplicates:</p>
<ul>
<li>the original, imported, PDF file</li>
<li>the unlocked PDF file (<code>qpdf</code>) which will render and print properly at least</li>
<li>the OCR’ed PDF file which has gained a text layer if the original didn’t have one (or has an incomplete or considered-wrong-by-the-user one)</li>
<li>the PDF file which has everything above plus embedded annotations perhaps? Or attachments? – Do we really want this variant in the library or do we wish to generate it on the fly every time a user requests it?</li>
</ul>
<p>As these variants all will have quite different content hashes, there’s the additional task for Qiqqa to link up multiple content hashes as ‘technical duplicates’ of the paper. Hence my take on this as calling this another type of (semi-)duplicate: all those semi-duplicates discussed above suffer from the same ailment: different content hashes for the same base content due to slew of different reasons.</p>
<p>Qiqqa currently (Oct 2020) only supports “one document, one content hash”, but this must be turned into something where multiple PDF files can be linked up and ‘clustered’ into a single overarching item, while the different <em>files</em> <em>will</em> have slightly different metadata in some places, e.g. their ‘state’ or ‘type’ or whatever you wish to call it:</p>
<ul>
<li>
<p>is this the imported original?</p>
</li>
<li>
<p>is this a copy of X, which has had superfluous leader pages stripped off?</p>
</li>
<li>
<p>is this a copy of X, which has been unlocked for regular viewing and printing?</p>
</li>
<li>
<p>is this a copy of X, which has a searchable text layer?</p>
<ul>
<li>
<p>has this text layer been vetted by the user as ‘good’/‘use this!’?</p>
<p>e.g. some (older) TeX output PDFs have a non-Unicode character encoding which does not convert to <strong>intelligible</strong> searchable text: it is quite useful to have these carry an additional or maybe altered text layer which can be extracted to produce legible text easily.</p>
<p>Some TeX files also seem to have lost all their whitespace in the text extract, so some more work needs to be done on the tools anyway and an additional text layer might help.</p>
</li>
<li>
<p>is this a text layer in a language that you can read / <em>comprehend</em>?</p>
<p>Then there are the ‘foreign’ publications, e.g. Chinese papers in my particualr case, which require <strong>language translation</strong> to become intelligible to me.
Once again, an additional text layer might come in handy to show me a translated overlay of the content. When this translation is stored in the PDF, we end up with a <em>technically different content</em> and thus a different file with a different content hash.</p>
</li>
</ul>
<p>And leading up to that, there’s:</p>
<ul>
<li>has this text layer been vetted/rated by the automation in Qiqqa? (Automatic rating / TOC building / …whatever…)</li>
<li>has this text layer been (auto-)translated by the automation in Qiqqa to your language(s) of choice?</li>
</ul>
<p>As the text layer would quite probably be obtained through the OCR process, we may also want to store/track the OCR and OCR preprocessing setup which resulted in this text layer: what have we done with the original to produce this?</p>
<p>Though one might want to store that sort of info in a separate process configuration file or database record, instead of including it <em>inside</em> the PDF; the resulting text layer <em>will</em> be part of the new PDF anyhow!</p>
</li>
</ul>
</li>
</ul>


    <footer>
      © 2020 Qiqqa Contributors ::
      <a href="https://github.com/GerHobbelt/qiqqa-open-source/blob/docs-src/Qiqqa Internals/Papers and their metadata, where to find papers, all about papers' aspects.md">Edit this page on GitHub</a>
    </footer>
  </body>
</html>
